{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1b4sHWZEQjuTOw2LXfXDMnFNnPC1zvDg0","authorship_tag":"ABX9TyN85NFoWxtIKgg6zIRasyHq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHVKuWq7wFBk","executionInfo":{"status":"ok","timestamp":1737295186396,"user_tz":-60,"elapsed":563,"user":{"displayName":"Vishnu Prasad","userId":"01466510163495346566"}},"outputId":"90d4fc8e-6f8b-4416-8062-4af0fce83680"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'debias_transformers' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/SoumyaBarikeri/debias_transformers.git\n"]},{"cell_type":"code","source":["\n","import os\n","os.chdir('/content/debias_transformers')\n"],"metadata":{"id":"IMYhxahcwZ6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -e .\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZvA4PPy807d","executionInfo":{"status":"ok","timestamp":1737295292155,"user_tz":-60,"elapsed":13004,"user":{"displayName":"Vishnu Prasad","userId":"01466510163495346566"}},"outputId":"f5a5a5c8-1afa-4d93-f17a-472eb39da4b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/debias_transformers\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (1.26.4)\n","Collecting tokenizers==0.8.1.rc2 (from transformers==3.3.0)\n","  Downloading tokenizers-0.8.1rc2.tar.gz (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (3.16.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (2.32.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (4.67.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (2024.11.6)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (0.2.0)\n","Collecting sacremoses (from transformers==3.3.0)\n","  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==3.3.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==3.3.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==3.3.0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==3.3.0) (2024.12.14)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses->transformers==3.3.0) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses->transformers==3.3.0) (1.4.2)\n","Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: transformers, tokenizers\n","  Building editable for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-3.3.0-0.editable-py3-none-any.whl size=9194 sha256=4a0071a04f7b61228a1037ae1984a3390c79706888493e2b4ace2b0779c78159\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-5q9qcdoj/wheels/09/ac/4d/583d811395149bf0163a645f3a79f5cb4735c5802776253c05\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully built transformers\n","Failed to build tokenizers\n","\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LgCvLMJYwpGB","executionInfo":{"status":"ok","timestamp":1737295334060,"user_tz":-60,"elapsed":23759,"user":{"displayName":"Vishnu Prasad","userId":"01466510163495346566"}},"outputId":"18499d26-fea8-4dd0-984c-cfc32c422391"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# List files in your Google Drive\n","data_path = '/content/drive/MyDrive/Reddit-Data'\n","print(os.listdir(data_path))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_PZYs5r_xbzU","executionInfo":{"status":"ok","timestamp":1737295338703,"user_tz":-60,"elapsed":577,"user":{"displayName":"Vishnu Prasad","userId":"01466510163495346566"}},"outputId":"fde1372f-d82e-440c-96c4-7beda9f210dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['reddit_comments_gender_female_raw_3.csv', 'gender_female.txt', 'reddit_comments_gender_female_raw_2.csv', 'reddit_comments_gender_female_raw_0.csv', 'reddit_comments_gender_female_raw_5.csv', 'reddit_comments_gender_female_raw_4.csv', 'reddit_comments_gender_female_raw_1.csv', 'gender', 'text_files', 'reddit_comments_race_black_raw_4.csv', 'reddit_comments_race_black_raw_3.csv', 'reddit_comments_race_black_raw_1.csv', 'reddit_comments_race_black_raw_0.csv', 'reddit_comments_race_black_raw_2.csv', 'race', 'race_black.txt', 'reddit_comments_orientation_lgbtq_raw_3.csv', 'reddit_comments_orientation_lgbtq_raw_4.csv', 'reddit_comments_orientation_lgbtq_raw_2.csv', 'reddit_comments_orientation_lgbtq_raw_0.csv', 'reddit_comments_orientation_lgbtq_raw_1.csv', 'reddit_comments_orientation_lgbtq_merged.csv', 'orientation', 'orientation_lgbtq.txt', 'reddit_comments_race_black_merged.csv', 'reddit_comments_gender_female_merged.csv', 'models']\n"]}]},{"cell_type":"code","source":["# Example: Load the gender training data file\n","gender_train_file = data_path+'/text_files/race/race_bias_manual_train.txt'\n","gender_bias_manual_swapped_targets_train_file='/content/drive/MyDrive/Reddit-Data/text_files/race/race_bias_manual_swapped_targets_train.txt'\n","# Read the contents of the file\n","with open(gender_train_file, 'r') as file:\n","    gender_data = file.readlines()\n","\n","# Print the first few lines\n","print(gender_data[:5])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIDIHrhjxh8s","executionInfo":{"status":"ok","timestamp":1737296324998,"user_tz":-60,"elapsed":1160,"user":{"displayName":"Vishnu Prasad","userId":"01466510163495346566"}},"outputId":"0a276cee-7224-43a5-a5e1-881103b5ed35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['black women are so violent and aggressive\\n', 'you know it isnt a coincidence that black people are discriminated against and make up of the prison.\\n', 'black women are loud af and disrespectful and trashy and id never get involved with one. there are a few exceptions though.\\n', 'and blacks are less intelligent?\\n', 'lol no wed be labelled terrorists and sent to jail. only lefties feminists and mobs of blacks are allowed to protest and vandalize and block traffic\\n']\n"]}]},{"cell_type":"code","source":["# coding=utf-8\n","\n","import logging\n","import math\n","from transformers import (\n","    AutoConfig,\n","    AutoModelWithLMHead,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    Trainer,\n","    set_seed,\n","    TrainingArguments\n",")\n","\n","logger = logging.getLogger(__name__)\n","\n","def main():\n","    # Hardcoded arguments\n","    output_dir = \"/content/debias_transformers/models/race/lm_loss_swapped_target/\"\n","    model_type = \"gpt2\"\n","    model_name_or_path = \"microsoft/DialoGPT-small\"\n","    config_name = \"microsoft/DialoGPT-small\"\n","    tokenizer_name = \"microsoft/DialoGPT-small\"\n","    train_data_file = gender_bias_manual_swapped_targets_train_file\n","    eval_data_file = \"/content/drive/MyDrive/Reddit-Data/text_files/humanref6k.txt\"\n","    num_train_epochs = 2.0\n","    save_total_limit = 2\n","    logging_steps = 2000\n","    save_steps = 2000\n","    per_device_train_batch_size = 2\n","    per_device_eval_batch_size = 2\n","    block_size = 36\n","    gradient_accumulation_steps = 1\n","    line_by_line = True\n","    force_pad_token = True\n","    overwrite_output_dir = True\n","    seed = 42\n","\n","    # Set seed\n","    set_seed(seed)\n","\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(config_name, cache_dir=None)\n","    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, cache_dir=None)\n","    model = AutoModelWithLMHead.from_pretrained(\n","        model_name_or_path, from_tf=bool(\".ckpt\" in model_name_or_path), config=config\n","    )\n","\n","    if force_pad_token:\n","        special_tokens_dict = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","        model.resize_token_embeddings(len(tokenizer))\n","\n","    # Load datasets\n","    from transformers import LineByLineTextDataset\n","    train_dataset = LineByLineTextDataset(\n","        tokenizer=tokenizer, file_path=train_data_file, block_size=block_size\n","    )\n","    eval_dataset = LineByLineTextDataset(\n","        tokenizer=tokenizer, file_path=eval_data_file, block_size=block_size\n","    )\n","\n","    # Data collator\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, mlm=False\n","    )\n","\n","    # Initialize trainer\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        overwrite_output_dir=overwrite_output_dir,\n","        num_train_epochs=num_train_epochs,\n","        per_device_train_batch_size=per_device_train_batch_size,\n","        per_device_eval_batch_size=per_device_eval_batch_size,\n","        save_steps=save_steps,\n","        logging_steps=logging_steps,\n","        save_total_limit=save_total_limit,\n","        gradient_accumulation_steps=gradient_accumulation_steps,\n","        report_to=\"none\",  # Disable W&B logging\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        data_collator=data_collator,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","    )\n","\n","    # Training\n","    trainer.train()\n","    trainer.save_model()\n","    tokenizer.save_pretrained(output_dir)\n","\n","    # Evaluation\n","    eval_output = trainer.evaluate()\n","    perplexity = math.exp(eval_output[\"eval_loss\"])\n","    print(f\"Perplexity: {perplexity}\")\n","\n","\n","main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113},"id":"JKJTkYa8RlXA","executionInfo":{"status":"ok","timestamp":1737297185540,"user_tz":-60,"elapsed":174430,"user":{"displayName":"Vishnu Prasad","userId":"01466510163495346566"}},"outputId":"fcf073cb-da3b-4c17-da8c-990792a1661c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1526' max='1526' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1526/1526 02:10, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3000/3000 00:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Perplexity: 640.2304769827537\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from scipy import stats\n","from transformers import (\n","    AutoModelForMaskedLM,\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n",")\n","import time\n","import logging\n","import math\n","import torch\n","\n","# Hardcoded arguments\n","data_path = \"/content/drive/MyDrive/Reddit-Data/\"\n","log_path = \"/content/drive/MyDrive/Reddit-Data/\"\n","GET_PERPLEXITY = \"yes\"\n","SAVE_PERPLEXITY = \"yes\"\n","demo = \"race\"\n","demo_1 = \"black\"\n","demo_2 = \"white\"\n","input_file_1 = \"reddit_comments_race_black_processed_phrase_biased_testset_reduced.csv\"\n","input_file_2 = \"reddit_comments_race_white_processed_phrase_biased_testset_reduced.csv\"\n","output_file_1 = \"reddit_comment_race_black_with_perplexity.csv\"\n","output_file_2 = \"reddit_commentss_race_white_with_perplexity.csv\"\n","pretrained_model = \"/content/debias_transformers/models/race/lm_loss_swapped_target/\"\n","debiasing_head = None\n","model_name = \"lm_loss_swapped_target\"\n","\n","pd.set_option(\"max_colwidth\", 600)\n","pd.options.display.max_columns = 10\n","logging.basicConfig(\n","    filename=log_path + \"measure_bias_\" + demo + \".log\",\n","    filemode=\"a\",\n","    level=logging.DEBUG,\n","    format=\"%(asctime)s %(message)s\",\n",")\n","\n","logging.info(\"Evaluating bias for model: {}\".format(model_name))\n","\n","# Perplexity functions\n","def perplexity_score(sentence):\n","    global model, tokenizer\n","    with torch.no_grad():\n","        model.eval()\n","        tokenize_input = tokenizer.tokenize(sentence)\n","        tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n","        loss = model(tensor_input, labels=tensor_input)\n","        return math.exp(loss[0])\n","\n","\n","def get_perplexity_list(df):\n","    perplexity_list = []\n","    for idx, row in df.iterrows():\n","        try:\n","            perplexity = perplexity_score(row[\"comments_processed\"])\n","        except Exception as ex:\n","            logging.error(f\"Error in perplexity calculation: {ex}\")\n","            perplexity = 0\n","        perplexity_list.append(perplexity)\n","    return perplexity_list\n","\n","\n","# Main execution\n","start = time.time()\n","\n","if GET_PERPLEXITY == \"yes\":\n","    logging.info(\"Calculating perplexity\")\n","    race_df = pd.read_csv(data_path + demo + \"/\" + input_file_1)\n","    race_df_2 = pd.read_csv(data_path + demo + \"/\" + input_file_2)\n","    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n","\n","    if debiasing_head:\n","        logging.info(\"Loading debiased model..\")\n","        model = AutoModelForMaskedLM.from_pretrained(pretrained_model, debiasing_head=debiasing_head)\n","    else:\n","        if \"bert\" in pretrained_model.lower():\n","            logging.info(\"Using BERT-based model\")\n","            model = AutoModelForMaskedLM.from_pretrained(pretrained_model)\n","        elif \"gpt\" in pretrained_model.lower():\n","            logging.info(\"Using GPT-based model\")\n","            model = AutoModelForCausalLM.from_pretrained(pretrained_model)\n","        else:\n","            logging.info(\"Using default causal language model\")\n","            model = AutoModelForCausalLM.from_pretrained(pretrained_model)\n","\n","    race_1_perplexity = get_perplexity_list(race_df)\n","    logging.info(\"Completed demo1 perplexity in {:.2f} minutes\".format((time.time() - start) / 60))\n","    race_2_perplexity = get_perplexity_list(race_df_2)\n","    logging.info(\"Completed demo2 perplexity in {:.2f} minutes\".format((time.time() - start) / 60))\n","\n","    race_df[\"perplexity\"] = race_1_perplexity\n","    race_df_2[\"perplexity\"] = race_2_perplexity\n","\n","    if SAVE_PERPLEXITY == \"yes\":\n","        logging.info(\"Saving perplexity results to files.\")\n","        race_df.to_csv(data_path + demo + \"/\" + output_file_1, index=False)\n","        race_df_2.to_csv(data_path + demo + \"/\" + output_file_2, index=False)\n","else:\n","    logging.info(\"Loading saved perplexities\")\n","    race_df = pd.read_csv(data_path + demo + \"/\" + output_file_1)\n","    race_df_2 = pd.read_csv(data_path + demo + \"/\" + output_file_2)\n","    race_1_perplexity = race_df[\"perplexity\"]\n","    race_2_perplexity = race_df_2[\"perplexity\"]\n","\n","logging.info(\n","    \"Mean and variance for demo1 perplexities: Mean = {:.4f}, Variance = {:.4f}\".format(\n","        np.mean(race_1_perplexity), np.var(race_1_perplexity)\n","    )\n",")\n","logging.info(\n","    \"Mean and variance for demo2 perplexities: Mean = {:.4f}, Variance = {:.4f}\".format(\n","        np.mean(race_2_perplexity), np.var(race_2_perplexity)\n","    )\n",")\n","\n","logging.info(\"Test samples: demo1 = {}, demo2 = {}\".format(len(race_1_perplexity), len(race_2_perplexity)))\n","\n","# Statistical testing\n","t_value, p_value = stats.ttest_ind(race_1_perplexity, race_2_perplexity, equal_var=False)\n","logging.info(\"Unpaired t-test: t-value = {:.4f}, p-value = {:.4f}\".format(t_value, p_value))\n","\n","t_paired, p_paired = stats.ttest_rel(race_df[\"perplexity\"].tolist(), race_df_2[\"perplexity\"].tolist())\n","logging.info(\"Paired t-test: t-value = {:.4f}, p-value = {:.4f}\".format(t_paired, p_paired))\n","\n","# Logging output to both file and console\n","console = logging.StreamHandler()\n","console.setLevel(logging.INFO)\n","formatter = logging.Formatter(\"%(asctime)s %(message)s\")\n","console.setFormatter(formatter)\n","logging.getLogger().addHandler(console)\n","\n","# Print final results to console\n","print(\"\\n--- Final Results ---\")\n","print(f\"Mean and variance for {demo_1} perplexities: Mean = {np.mean(race_1_perplexity):.4f}, Variance = {np.var(race_1_perplexity):.4f}\")\n","print(f\"Mean and variance for {demo_2} perplexities: Mean = {np.mean(race_2_perplexity):.4f}, Variance = {np.var(race_2_perplexity):.4f}\")\n","\n","print(f\"Unpaired t-test: t-value = {t_value:.4f}, p-value = {p_value:.4f}\")\n","print(f\"Paired t-test: t-value = {t_paired:.4f}, p-value = {p_paired:.4f}\")\n","\n","print(f\"Test samples: {demo_1} = {len(race_1_perplexity)}, {demo_2} = {len(race_2_perplexity)}\")\n","print(\"\\n----------------------\")\n"],"metadata":{"id":"DGkLkiVnbfUl"},"execution_count":null,"outputs":[]}]}