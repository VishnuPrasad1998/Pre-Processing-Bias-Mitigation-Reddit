{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8961,
     "status": "ok",
     "timestamp": 1742051885349,
     "user": {
      "displayName": "Vishnu Prasad",
      "userId": "01466510163495346566"
     },
     "user_tz": -60
    },
    "id": "hHVKuWq7wFBk",
    "outputId": "7d1925b7-5e4e-4d63-cce5-f6491e855fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'debias_transformers'...\n",
      "remote: Enumerating objects: 44565, done.\u001b[K\n",
      "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 44565 (delta 4), reused 3 (delta 3), pack-reused 44555 (from 1)\u001b[K\n",
      "Receiving objects: 100% (44565/44565), 38.49 MiB | 9.30 MiB/s, done.\n",
      "Resolving deltas: 100% (30346/30346), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SoumyaBarikeri/debias_transformers.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMYhxahcwZ6y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/debias_transformers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28685,
     "status": "ok",
     "timestamp": 1742052067359,
     "user": {
      "displayName": "Vishnu Prasad",
      "userId": "01466510163495346566"
     },
     "user_tz": -60
    },
    "id": "JZvA4PPy807d",
    "outputId": "ee705e4f-2548-4bd2-bec1-0cfa4be144e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/debias_transformers\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (1.26.4)\n",
      "Collecting tokenizers==0.8.1.rc2 (from transformers==3.3.0)\n",
      "  Using cached tokenizers-0.8.1rc2.tar.gz (97 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (3.17.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (2024.11.6)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.11/dist-packages (from transformers==3.3.0) (0.2.0)\n",
      "Collecting sacremoses (from transformers==3.3.0)\n",
      "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==3.3.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==3.3.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==3.3.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==3.3.0) (2025.1.31)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses->transformers==3.3.0) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses->transformers==3.3.0) (1.4.2)\n",
      "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "Building wheels for collected packages: transformers, tokenizers\n",
      "  Building editable for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-3.3.0-0.editable-py3-none-any.whl size=9194 sha256=087e146284f820474214e1725f2bdd0d5a6fc796d177165604e39b72958ec458\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-t6_qo6r7/wheels/09/ac/4d/583d811395149bf0163a645f3a79f5cb4735c5802776253c05\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully built transformers\n",
      "Failed to build tokenizers\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14805,
     "status": "ok",
     "timestamp": 1742052082166,
     "user": {
      "displayName": "Vishnu Prasad",
      "userId": "01466510163495346566"
     },
     "user_tz": -60
    },
    "id": "LgCvLMJYwpGB",
    "outputId": "87a00c83-a74e-4469-9c0c-4f7f3c7335d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1280,
     "status": "ok",
     "timestamp": 1742052083439,
     "user": {
      "displayName": "Vishnu Prasad",
      "userId": "01466510163495346566"
     },
     "user_tz": -60
    },
    "id": "_PZYs5r_xbzU",
    "outputId": "4aba86d9-3e8d-4368-982e-30534af75a16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reddit_comments_gender_female_raw_3.csv', 'gender_female.txt', 'reddit_comments_gender_female_raw_2.csv', 'reddit_comments_gender_female_raw_0.csv', 'reddit_comments_gender_female_raw_5.csv', 'reddit_comments_gender_female_raw_4.csv', 'reddit_comments_gender_female_raw_1.csv', 'gender', 'text_files', 'reddit_comments_race_black_raw_4.csv', 'reddit_comments_race_black_raw_3.csv', 'reddit_comments_race_black_raw_1.csv', 'reddit_comments_race_black_raw_0.csv', 'reddit_comments_race_black_raw_2.csv', 'race', 'race_black.txt', 'reddit_comments_orientation_lgbtq_raw_3.csv', 'reddit_comments_orientation_lgbtq_raw_4.csv', 'reddit_comments_orientation_lgbtq_raw_2.csv', 'reddit_comments_orientation_lgbtq_raw_0.csv', 'reddit_comments_orientation_lgbtq_raw_1.csv', 'reddit_comments_orientation_lgbtq_merged.csv', 'orientation', 'orientation_lgbtq.txt', 'reddit_comments_race_black_merged.csv', 'reddit_comments_gender_female_merged.csv', 'models', 'race_bias_manual_swapped_attr_train.txt']\n"
     ]
    }
   ],
   "source": [
    "# List files in your Google Drive\n",
    "data_path = '/content/drive/MyDrive/Reddit-Data'\n",
    "print(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1936,
     "status": "ok",
     "timestamp": 1742052085377,
     "user": {
      "displayName": "Vishnu Prasad",
      "userId": "01466510163495346566"
     },
     "user_tz": -60
    },
    "id": "MIDIHrhjxh8s",
    "outputId": "8cb55b41-2afa-4700-d171-5cacd193f63d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the dance in that vine where that girl is dancing to the fire alarm\\n', 'gonna bang her as soon as your mom is done cleaning her snizz off my\\n', 'insinuating that hug from mom is not better than shakespeare. gabdisgust\\n', 'doras supposedly-well-endowed mom is comparable to  tai in modern\\n', 'dance moms are literally demons in honda civics source\\n']\n"
     ]
    }
   ],
   "source": [
    "# Example: Load the gender training data file\n",
    "gender_train_file = data_path+'/text_files/gender/gender_bias_manual_train.txt'\n",
    "gender_bias_manual_swapped_targets_train_file='/content/drive/MyDrive/Reddit-Data/text_files/gender/gender_bias_manual_swapped_targets_train.txt'\n",
    "# Read the contents of the file\n",
    "with open(gender_train_file, 'r') as file:\n",
    "    gender_data = file.readlines()\n",
    "\n",
    "# Print the first few lines\n",
    "print(gender_data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574,
     "referenced_widgets": [
      "2e012318604c44fa8a32acaa8ce24f2a",
      "a6bad7f0f91f45ac9e69ff1c9cdbf44c",
      "b02d21e4292847578b60987bc9ea82cf",
      "1cded95826094d8ea6726644d86372b2",
      "484e61d6e4da43d386bab92fa061e1e6",
      "95f2d0a94e724bb1a7ff0fd6593428e6",
      "d46f057e26844864a90340d2a343d62e",
      "c1bd6dbb059c421c9529b7f10de4d923",
      "10d01a6efc76431facf36fab507fd92f",
      "033c39f716e24154834d90e86a5888cc",
      "8adf8f9a5dc64d90a1ae99dcdaf5bf13",
      "c9a7496acb534724b0162c3b3f4249a3",
      "ea06a5b88897424ea6ff9718f3ee9940",
      "2abb80d9e26d4bd4916da86954f20388",
      "34941e2868414622af508fa646e94e1d",
      "29c5e3110d27471988bebfdeba8d520a",
      "460a39fb461846918224894c8b74aa69",
      "60f00ae983b6422889acbe252c13ce5a",
      "6facbd4686074b0a9f20d91786c40263",
      "8887eb40a9cb4bdda9de079e5b0d9ce2",
      "0d75eacdee30494e8226e2fad384a419",
      "b14474fe854b4c3ea467c39d03ba7de6",
      "df061caa14fe4bcd91c0d06eff171432",
      "4eb86d1a579c4d59965d1ddc5aa9145f",
      "0ac3b02f252144ed8a31f2adb59febb9",
      "baa261b09536447bacee32fed9ebfa4a",
      "d14a5e5405d84bb086a54eeb101e317c",
      "a30f079e6e444867b28a96468854c8ba",
      "901dcd91994f4fceaaff4b79d6417953",
      "4959206d185d4644bccecfe8a25e923b",
      "a4bea1cdaca24bdc9ab9e225cc67ce00",
      "caa6133573394cb2a0a49b94ee674d01",
      "6b418297c63248498ea055200585d0ef",
      "263ec0e0e4b447abbabe4e636399c009",
      "8787074f1ed04fb7a52553fcb3ef2790",
      "a96e164ce5cb4ca88a3d14b58f6f793f",
      "28f522146aa84a2b9091078dd296a282",
      "8e3846a5ceba4972b68ce44ef6f78ff4",
      "bf21fd1d8167473c8cda870e8d1ad43d",
      "5d4dd02be7414a3db27c3d09b5b39207",
      "f5d19ed3f5984550999c3682b0e80e7a",
      "abea98a269e34cf8bccb6189b99a3b7f",
      "bf3645ef98054fb18ac7cd4062250591",
      "0ee06099073e4d708ef442f3466a0d2e",
      "2ae365464be84d5495eeb66137675cd1",
      "e869f826a80044309c4d74ff861688ce",
      "d5c0a505a2cd46219556c0a1c39c714c",
      "a4d4b2dba868408ea3d8545d5959dd71",
      "26722eb2865c40368282350a28b43332",
      "0d7019dccabd4c17afb29c7ae04d0ae1",
      "d545eca537be4d99a3e7c9e7cda9ab77",
      "ef8afd8e5d01486892e5a70e49dc257b",
      "e5a2654169804b07bbbe541d50e968a2",
      "262b2e67f24146949d0ebf18d4bc3c44",
      "0572b3ed30eb4e8fb5a9632ea3989540",
      "461ae2b82dc048b6ae04d82985a00ff3",
      "0c5394c058b7417ca9bab723cde28e36",
      "2b6ad8a2e8df474db83cf16f508b5be9",
      "796c2fe0bf364ec9a4774bcf7b878235",
      "ef51e55acfaf400d8e72c449d5d0eb28",
      "cf558856093d4aceaede6213a91e58e1",
      "73178012097745fdb59ee798f8f7709b",
      "0a344e8864fc478c99e5f74db1737c14",
      "a06f1151a8054d55b57c96224d8a8054",
      "549f9f349270437eac95b6e5b66a8aa0",
      "524e13e48d5547068f94cdddffedbaab"
     ]
    },
    "executionInfo": {
     "elapsed": 381388,
     "status": "ok",
     "timestamp": 1742052466766,
     "user": {
      "displayName": "Vishnu Prasad",
      "userId": "01466510163495346566"
     },
     "user_tz": -60
    },
    "id": "JKJTkYa8RlXA",
    "outputId": "ac1022ec-732c-44df-9c79-7ce75660b222"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e012318604c44fa8a32acaa8ce24f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a7496acb534724b0162c3b3f4249a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df061caa14fe4bcd91c0d06eff171432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263ec0e0e4b447abbabe4e636399c009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/modeling_auto.py:1881: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae365464be84d5495eeb66137675cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461ae2b82dc048b6ae04d82985a00ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3042' max='3042' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3042/3042 04:38, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.288900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 675.6247879955285\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import logging\n",
    "import math\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    # Hardcoded arguments\n",
    "    output_dir = \"/content/debias_transformers/models/gender/lm_loss_swapped_target/\"\n",
    "    model_type = \"gpt2\"\n",
    "    model_name_or_path = \"microsoft/DialoGPT-small\"\n",
    "    config_name = \"microsoft/DialoGPT-small\"\n",
    "    tokenizer_name = \"microsoft/DialoGPT-small\"\n",
    "    train_data_file = gender_bias_manual_swapped_targets_train_file\n",
    "    eval_data_file = \"/content/drive/MyDrive/Reddit-Data/text_files/humanref6k.txt\"\n",
    "\n",
    "    num_train_epochs = 2.0\n",
    "    save_total_limit = 2\n",
    "    logging_steps = 2000\n",
    "    save_steps = 2000\n",
    "    per_device_train_batch_size = 2\n",
    "    per_device_eval_batch_size = 2\n",
    "    block_size = 36\n",
    "    gradient_accumulation_steps = 1\n",
    "    seed = 42\n",
    "\n",
    "    line_by_line = True\n",
    "    force_pad_token = True\n",
    "    overwrite_output_dir = True\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    config = AutoConfig.from_pretrained(config_name, cache_dir=None)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, cache_dir=None)\n",
    "    model = AutoModelWithLMHead.from_pretrained(\n",
    "        model_name_or_path, from_tf=bool(\".ckpt\" in model_name_or_path), config=config\n",
    "    )\n",
    "\n",
    "    if force_pad_token:\n",
    "        special_tokens_dict = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>'}\n",
    "        tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Load datasets\n",
    "    from transformers import LineByLineTextDataset\n",
    "    train_dataset = LineByLineTextDataset(\n",
    "        tokenizer=tokenizer, file_path=train_data_file, block_size=block_size\n",
    "    )\n",
    "    eval_dataset = LineByLineTextDataset(\n",
    "        tokenizer=tokenizer, file_path=eval_data_file, block_size=block_size\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        save_steps=save_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        save_total_limit=save_total_limit,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        report_to=\"none\",  # Disable W&B logging\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    eval_output = trainer.evaluate()\n",
    "    perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "    print(f\"Perplexity: {perplexity}\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297312,
     "status": "ok",
     "timestamp": 1742052764081,
     "user": {
      "displayName": "Vishnu Prasad",
      "userId": "01466510163495346566"
     },
     "user_tz": -60
    },
    "id": "DGkLkiVnbfUl",
    "outputId": "d434551f-662f-405f-ad31-c97c6d453a53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Results ---\n",
      "Mean and variance for female perplexities: Mean = 284.0309, Variance = 324722.9097\n",
      "Mean and variance for male perplexities: Mean = 293.0374, Variance = 318604.4102\n",
      "Unpaired t-test: t-value = -0.3348, p-value = 0.7378\n",
      "Paired t-test: t-value = -2.2697, p-value = 0.0235\n",
      "Test samples: female = 890, male = 890\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import time\n",
    "import logging\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# Hardcoded arguments\n",
    "data_path = \"/content/drive/MyDrive/Reddit-Data/\"\n",
    "log_path = \"/content/drive/MyDrive/Reddit-Data/\"\n",
    "GET_PERPLEXITY = \"yes\"\n",
    "SAVE_PERPLEXITY = \"yes\"\n",
    "demo = \"gender\"\n",
    "demo_1 = \"female\"\n",
    "demo_2 = \"male\"\n",
    "input_file_1 = \"reddit_comments_gender_female_processed_phrase_biased_testset_reduced.csv\"\n",
    "input_file_2 = \"reddit_comments_gender_male_processed_phrase_biased_testset_reduced.csv\"\n",
    "output_file_1 = \"reddit_comments_gender_female_with_perplexity.csv\"\n",
    "output_file_2 = \"reddit_comments_gender_male_with_perplexity.csv\"\n",
    "pretrained_model = \"/content/debias_transformers/models/gender/lm_loss_swapped_target/\"\n",
    "debiasing_head = None\n",
    "model_name = \"lm_loss_swapped_target\"\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 600)\n",
    "pd.options.display.max_columns = 10\n",
    "logging.basicConfig(\n",
    "    filename=log_path + \"measure_bias_\" + demo + \".log\",\n",
    "    filemode=\"a\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    ")\n",
    "\n",
    "logging.info(\"Evaluating bias for model: {}\".format(model_name))\n",
    "\n",
    "# Perplexity functions\n",
    "def perplexity_score(sentence):\n",
    "    global model, tokenizer\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        tokenize_input = tokenizer.tokenize(sentence)\n",
    "        tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "        loss = model(tensor_input, labels=tensor_input)\n",
    "        return math.exp(loss[0])\n",
    "\n",
    "\n",
    "def get_perplexity_list(df):\n",
    "    perplexity_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            perplexity = perplexity_score(row[\"comments_processed\"])\n",
    "        except Exception as ex:\n",
    "            logging.error(f\"Error in perplexity calculation: {ex}\")\n",
    "            perplexity = 0\n",
    "        perplexity_list.append(perplexity)\n",
    "    return perplexity_list\n",
    "\n",
    "\n",
    "# Main execution\n",
    "start = time.time()\n",
    "\n",
    "if GET_PERPLEXITY == \"yes\":\n",
    "    logging.info(\"Calculating perplexity\")\n",
    "    gender_df = pd.read_csv(data_path + demo + \"/\" + input_file_1)\n",
    "    gender_df_2 = pd.read_csv(data_path + demo + \"/\" + input_file_2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "    if debiasing_head:\n",
    "        logging.info(\"Loading debiased model..\")\n",
    "        model = AutoModelForMaskedLM.from_pretrained(pretrained_model, debiasing_head=debiasing_head)\n",
    "    else:\n",
    "        if \"bert\" in pretrained_model.lower():\n",
    "            logging.info(\"Using BERT-based model\")\n",
    "            model = AutoModelForMaskedLM.from_pretrained(pretrained_model)\n",
    "        elif \"gpt\" in pretrained_model.lower():\n",
    "            logging.info(\"Using GPT-based model\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(pretrained_model)\n",
    "        else:\n",
    "            logging.info(\"Using default causal language model\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(pretrained_model)\n",
    "\n",
    "    gender_1_perplexity = get_perplexity_list(gender_df)\n",
    "    logging.info(\"Completed demo1 perplexity in {:.2f} minutes\".format((time.time() - start) / 60))\n",
    "    gender_2_perplexity = get_perplexity_list(gender_df_2)\n",
    "    logging.info(\"Completed demo2 perplexity in {:.2f} minutes\".format((time.time() - start) / 60))\n",
    "\n",
    "    gender_df[\"perplexity\"] = gender_1_perplexity\n",
    "    gender_df_2[\"perplexity\"] = gender_2_perplexity\n",
    "\n",
    "    if SAVE_PERPLEXITY == \"yes\":\n",
    "        logging.info(\"Saving perplexity results to files.\")\n",
    "        gender_df.to_csv(data_path + demo + \"/\" + output_file_1, index=False)\n",
    "        gender_df_2.to_csv(data_path + demo + \"/\" + output_file_2, index=False)\n",
    "else:\n",
    "    logging.info(\"Loading saved perplexities\")\n",
    "    gender_df = pd.read_csv(data_path + demo + \"/\" + output_file_1)\n",
    "    gender_df_2 = pd.read_csv(data_path + demo + \"/\" + output_file_2)\n",
    "    gender_1_perplexity = gender_df[\"perplexity\"]\n",
    "    gender_2_perplexity = gender_df_2[\"perplexity\"]\n",
    "\n",
    "logging.info(\n",
    "    \"Mean and variance for demo1 perplexities: Mean = {:.4f}, Variance = {:.4f}\".format(\n",
    "        np.mean(gender_1_perplexity), np.var(gender_1_perplexity)\n",
    "    )\n",
    ")\n",
    "logging.info(\n",
    "    \"Mean and variance for demo2 perplexities: Mean = {:.4f}, Variance = {:.4f}\".format(\n",
    "        np.mean(gender_2_perplexity), np.var(gender_2_perplexity)\n",
    "    )\n",
    ")\n",
    "\n",
    "logging.info(\"Test samples: demo1 = {}, demo2 = {}\".format(len(gender_1_perplexity), len(gender_2_perplexity)))\n",
    "\n",
    "# Statistical testing\n",
    "t_value, p_value = stats.ttest_ind(gender_1_perplexity, gender_2_perplexity, equal_var=False)\n",
    "logging.info(\"Unpaired t-test: t-value = {:.4f}, p-value = {:.4f}\".format(t_value, p_value))\n",
    "\n",
    "t_paired, p_paired = stats.ttest_rel(gender_df[\"perplexity\"].tolist(), gender_df_2[\"perplexity\"].tolist())\n",
    "logging.info(\"Paired t-test: t-value = {:.4f}, p-value = {:.4f}\".format(t_paired, p_paired))\n",
    "\n",
    "# Logging output to both file and console\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s %(message)s\")\n",
    "console.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# Print final results to console\n",
    "print(\"\\n--- Final Results ---\")\n",
    "print(f\"Mean and variance for {demo_1} perplexities: Mean = {np.mean(gender_1_perplexity):.4f}, Variance = {np.var(gender_1_perplexity):.4f}\")\n",
    "print(f\"Mean and variance for {demo_2} perplexities: Mean = {np.mean(gender_2_perplexity):.4f}, Variance = {np.var(gender_2_perplexity):.4f}\")\n",
    "\n",
    "print(f\"Unpaired t-test: t-value = {t_value:.4f}, p-value = {p_value:.4f}\")\n",
    "print(f\"Paired t-test: t-value = {t_paired:.4f}, p-value = {p_paired:.4f}\")\n",
    "\n",
    "print(f\"Test samples: {demo_1} = {len(gender_1_perplexity)}, {demo_2} = {len(gender_2_perplexity)}\")\n",
    "print(\"\\n----------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apj9uNnct37b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPDMqfksV+nbLTwtDqhrgz/",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1gHw66IVmrAAo9grLqTmHBuQ18irzsW6t",
     "timestamp": 1742051787282
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
